{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3604672",
   "metadata": {},
   "source": [
    "# TEAM NAME: Classification Predict\n",
    "## Introduction\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "### The task\n",
    "With this context, we've been tasked with creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "### The benefit\n",
    "The goal is to provide an accurate and robust solution that gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d23bd",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "The target variable for this classification is the sentiment of a tweet based on the author's views regarding climate change.\n",
    "\n",
    "We hypothesise that:\n",
    "1. Denialists may use negative terms such `lies`, `exposed`, `conspiracy`, `fake` more frequently\n",
    "2. Pro-climate change individuals use words such as `climate`, `global warming`, `earth`, `biosphere`, `global`, `fight`, `crisis`, `threat`, `denial` more often.\n",
    "3. News media will mostly share URL links to full articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6229b29e",
   "metadata": {},
   "source": [
    "### Data and Library Imports\n",
    "Now we will import the libraries required to perform:\n",
    "* language manipulation\n",
    "* data import, manipulation and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9207d290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Stanford\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library imports\n",
    "\n",
    "# running experiments\n",
    "from comet_ml import Experiment\n",
    "# Data manipulation and visualisation\n",
    "import numpy as np # mathematical processing\n",
    "import pandas as pd # data manipulation\n",
    "import seaborn as sns # data visualisation\n",
    "import matplotlib.pyplot as plt  # data visualisation\n",
    "%matplotlib inline\n",
    "# set plot style\n",
    "sns.set()\n",
    "\n",
    "# Text manipulation\n",
    "import nltk # toolkit for language processing\n",
    "from nltk.corpus import stopwords # redundant words\n",
    "import re # regular expression for text extraction\n",
    "import string # for punctuation removal\n",
    "from nltk.tokenize import TreebankWordTokenizer # tokenizing words\n",
    "from nltk import SnowballStemmer # stemming tool\n",
    "from nltk import WordNetLemmatizer # lemmatizing tool\n",
    "from sklearn.feature_extraction.text import CountVectorizer # create vectorised word counts\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# model building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad8dd08",
   "metadata": {},
   "source": [
    "Next we will import the `test` and `train` data provided. Thereafter we will inspect the first 5 rows of the train data to get an understanding of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b035ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data importation\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e834ceff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 rows of train data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5330d8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe will now be looking to China to make su...</td>\n",
       "      <td>169760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Combine this with the polling of staffers re c...</td>\n",
       "      <td>35326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The scary, unimpeachable evidence that climate...</td>\n",
       "      <td>224985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Karoli @morgfair @OsborneInk @dailykos \\nPuti...</td>\n",
       "      <td>476263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @FakeWillMoore: 'Female orgasms cause globa...</td>\n",
       "      <td>872928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tweetid\n",
       "0  Europe will now be looking to China to make su...   169760\n",
       "1  Combine this with the polling of staffers re c...    35326\n",
       "2  The scary, unimpeachable evidence that climate...   224985\n",
       "3  @Karoli @morgfair @OsborneInk @dailykos \\nPuti...   476263\n",
       "4  RT @FakeWillMoore: 'Female orgasms cause globa...   872928"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 rows of test data\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3ace2",
   "metadata": {},
   "source": [
    "#### Data Description\n",
    "The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The datasets, consisting of `Train` and `Test` data aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. The data was downloaded from: https://www.kaggle.com/c/edsa-climate-change-belief-analysis-2022/data\n",
    "\n",
    "Each tweet is labelled as one of the following classes:\n",
    "\n",
    "Class Description\n",
    "* 2 News: the tweet links to factual news about climate change\n",
    "* 1 Pro: the tweet supports the belief of man-made climate change\n",
    "* 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "* -1 Anti: the tweet does not believe in man-made climate change\n",
    "\n",
    "Variable definitions\n",
    "- sentiment: Sentiment of tweet\n",
    "- message: Tweet body\n",
    "- tweetid: Twitter unique id\n",
    "\n",
    "Let's confirm that the sentiment values conform to the description above and next obtain a view of the data samples for each sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40ede91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 0, -1]\n"
     ]
    }
   ],
   "source": [
    "# identify the sentiment values\n",
    "sentiment_labels = list(train.sentiment.unique())\n",
    "print(sentiment_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26dc42db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD7CAYAAACCEpQdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbLklEQVR4nO3df0yd5f3/8ecp50irdNnozi2MMHTOzAVcMbIfzO0wjeGcimd1Jy5TGkk0M0omLtWxEGAwthmrQdiMO8Rk6mI1cVQtpyVnh23WkBlqRP6wYaJZHBCF7nCobAUsFM65P3/s2/Mt1vaAPYfD6f16JOZ4X/fF4breti/vc51z7stmmqaJiIhYyqZ0D0BERNafwl9ExIIU/iIiFqTwFxGxIIW/iIgFKfxFRCxI4S8iYkH2dA9gtWZm5onFNvZXErZty+HYsbl0D+OCoXomj2qZXJlQz02bbHzuc5ec9XzGhH8sZm748AcyYoyZRPVMHtUyuTK9nlr2ERGxIIW/iIgFKfxFRCxI4S8iYkEKfxERC1L4i4hYkMJfRMSCMuZz/sm29TNb2Jyd/Ok7nVuT+nwLi8vMHj+R1OcUEbFs+G/OtuN9MJDuYSR08LGdzKZ7ECJywdGyj4iIBSn8RUQsSOEvImJBCn8REQtS+IuIWJDCX0TEghT+IiIWtKrwDwQCVFVVUVVVxSOPPALAwMAAXq+XyspKOjs7431HRkbw+Xy43W6amppYXl4GYHJykl27duHxeKitrWV+fj4F0xERkdVIGP4nTpzgoYceYu/evQQCAd58800OHTpEY2Mjfr+fYDDI8PAw/f39ANTX19PS0kJfXx+madLd3Q1AW1sb1dXVhEIhSkpK8Pv9qZ2ZiIicVcLwj0ajxGIxTpw4wfLyMsvLy+Tk5FBUVERhYSF2ux2v10soFGJiYoKFhQVKS0sB8Pl8hEIhlpaWGBwcxO12r2gXEZH0SHh7h5ycHH7605+yY8cOtmzZwte//nWmpqZwOp3xPoZhEA6Hz2h3Op2Ew2FmZmbIycnBbrevaBcRkfRIGP7vvPMOL730Eq+++ipbt27lZz/7GWNjY9hstngf0zSx2WzEYrFPbD/1eLqPHyeybVvOmvpfSJJ9s7hMYuW5J5tqmVyZXs+E4f/aa69RXl7Otm3bgP8t2Tz11FNkZWXF+0QiEQzDIC8vj0gkEm+fnp7GMAxyc3OZnZ0lGo2SlZUV778Wx47NEYuZa/qZc8mk/3CRiDVv7eZ0brXs3JNNtUyuTKjnpk22c140J1zzv+qqqxgYGOCjjz7CNE0OHTrE9u3bGR0dZXx8nGg0Sm9vLy6Xi4KCArKzsxkaGgL+9ykhl8uFw+GgrKyMYDAIQE9PDy6XK0lTFBGRtUp45f+d73yHt99+G5/Ph8Ph4Oqrr6auro7rrruOuro6FhcXqaiowOPxANDe3k5zczNzc3MUFxdTU1MDQGtrKw0NDXR1dZGfn09HR0dqZyYiImdlM00zeWspKZSKZZ9MuZ//Rn95mSqZ8NI6U6iWyZUJ9TzvZR8REbnwKPxFRCxI4S8iYkEKfxERC1L4i4hYkMJfRMSCFP4iIhak8BcRsSCFv4iIBSn8RUQsSOEvImJBCn8REQtS+IuIWJDCX0TEghT+IiIWpPAXEbGghDt57du3j+eeey5+/MEHH7Bz505uvPFGHn74YRYXF9mxYwe7d+8GYGRkhKamJubn5ykrK6OtrQ273c7k5CT19fUcO3aMyy+/nPb2di655JLUzUxERM4q4ZX/D3/4QwKBAIFAgPb2drZt28bdd99NY2Mjfr+fYDDI8PAw/f39ANTX19PS0kJfXx+madLd3Q1AW1sb1dXVhEIhSkpK8Pv9qZ2ZiIic1ZqWfX75y1+ye/du3n//fYqKiigsLMRut+P1egmFQkxMTLCwsEBpaSkAPp+PUCjE0tISg4ODuN3uFe0iIpIeCZd9ThkYGGBhYYEdO3bQ29uL0+mMnzMMg3A4zNTU1Ip2p9NJOBxmZmaGnJwc7Hb7iva1ONdelBc6p3NruoeQNlaee7KplsmV6fVcdfi/8MIL3HnnnQDEYjFsNlv8nGma2Gy2s7afejzdx48TScUG7plio28UnSqZsEl2plAtkysT6pmUDdxPnjzJ4OAgN9xwAwB5eXlEIpH4+UgkgmEYZ7RPT09jGAa5ubnMzs4SjUZX9BcRkfRYVfi/++67XHbZZVx88cUAbN++ndHRUcbHx4lGo/T29uJyuSgoKCA7O5uhoSEAAoEALpcLh8NBWVkZwWAQgJ6eHlwuV4qmJCIiiaxq2ef9998nLy8vfpydnc2ePXuoq6tjcXGRiooKPB4PAO3t7TQ3NzM3N0dxcTE1NTUAtLa20tDQQFdXF/n5+XR0dKRgOiIisho20zSTt5CeQqlY8/c+GEja86XKwcd2bvi1xVTJhHXVTKFaJlcm1DMpa/4iInJhUfiLiFiQwl9ExIIU/iIiFqTwFxGxIIW/iIgFKfxFRCxI4S8iYkEKfxERC1L4i4hYkMJfRMSCFP4iIhak8BcRsSCFv4iIBSn8RUQsaFXhf+jQIXw+Hzt27OA3v/kN8L8N3b1eL5WVlXR2dsb7joyM4PP5cLvdNDU1sby8DMDk5CS7du3C4/FQW1vL/Px8CqYjIiKrkTD833//fVpbW/H7/Rw4cIC3336b/v5+Ghsb8fv9BINBhoeH6e/vB6C+vp6Wlhb6+vowTZPu7m4A2traqK6uJhQKUVJSgt/vT+3MRETkrBKG/1//+lduuukm8vLycDgcdHZ2smXLFoqKiigsLMRut+P1egmFQkxMTLCwsEBpaSkAPp+PUCjE0tISg4ODuN3uFe0iIpIeCffwHR8fx+FwcO+993L06FG+973vceWVV+J0OuN9DMMgHA4zNTW1ot3pdBIOh5mZmSEnJwe73b6iXURE0iNh+EejUd5880327t3LxRdfTG1tLZs3b8Zms8X7mKaJzWYjFot9Yvupx9N9/DiRc+1FeaFzOremewhpY+W5J5tqmVyZXs+E4f/5z3+e8vJycnNzAbjxxhsJhUJkZWXF+0QiEQzDIC8vj0gkEm+fnp7GMAxyc3OZnZ0lGo2SlZUV778WqdjAPVNs9I2iUyUTNsnOFKplcmVCPc97A/frr7+e1157jePHjxONRvn73/+Ox+NhdHSU8fFxotEovb29uFwuCgoKyM7OZmhoCIBAIIDL5cLhcFBWVkYwGASgp6cHl8uVpCmKiMhaJbzy3759Oz/+8Y+prq5maWmJ6667jttvv50vfelL1NXVsbi4SEVFBR6PB4D29naam5uZm5ujuLiYmpoaAFpbW2loaKCrq4v8/Hw6OjpSOzMRETkrm2mayVtLSaFULPt4Hwwk7flS5eBjOzf8y8tUyYSX1plCtUyuTKjneS/7iIjIhUfhLyJiQQp/ERELUviLiFiQwl9ExIIU/iIiFqTwFxGxIIW/iIgFKfxFRCxI4S8iYkEKfxERC1L4i4hYkMJfRMSCFP4iIhak8BcRsaCEm7kA3HHHHXz44YfxDdh/9atfMT8/z8MPP8zi4iI7duxg9+7dAIyMjNDU1MT8/DxlZWW0tbVht9uZnJykvr6eY8eOcfnll9Pe3s4ll1ySupmJiMhZJbzyN02TsbExAoFA/J+vfOUrNDY24vf7CQaDDA8P09/fD0B9fT0tLS309fVhmibd3d0AtLW1UV1dTSgUoqSkBL/fn9qZiYjIWSUM/3/9618A3HXXXXz/+9/nueee48iRIxQVFVFYWIjdbsfr9RIKhZiYmGBhYYHS0lIAfD4foVCIpaUlBgcHcbvdK9pFRCQ9Eob/8ePHKS8v5/e//z1//OMfeeGFF5icnMTpdMb7GIZBOBxmampqRbvT6SQcDjMzM0NOTk582ehUu4iIpEfCNf9rrrmGa665Jn5866238vjjj3PttdfG20zTxGazEYvFsNlsZ7Sfejzdx48TOddelBc6p3NruoeQNlaee7KplsmV6fVMGP5vvvkmS0tLlJeXA/8L9IKCAiKRSLxPJBLBMAzy8vJWtE9PT2MYBrm5uczOzhKNRsnKyor3X4tUbOCeKTb6RtGpkgmbZGcK1TK5MqGe572B++zsLI8++iiLi4vMzc2xf/9+HnjgAUZHRxkfHycajdLb24vL5aKgoIDs7GyGhoYACAQCuFwuHA4HZWVlBINBAHp6enC5XEmaooiIrFXCK//rr7+et956i1tuuYVYLEZ1dTXXXHMNe/bsoa6ujsXFRSoqKvB4PAC0t7fT3NzM3NwcxcXF1NTUANDa2kpDQwNdXV3k5+fT0dGR2pmJiMhZ2UzTTN5aSgqlYtnH+2Agac+XKgcf27nhX16mSia8tM4UqmVyZUI9z3vZR0RELjwKfxERC1L4i4hYkMJfRMSCFP4iIhak8BcRsSCFv4iIBSn8RUQsSOEvImJBCn8REQtS+IuIWJDCX0TEghT+IiIWpPAXEbEghb+IiAUp/EVELGjV4f/II4/Q0NAAwMDAAF6vl8rKSjo7O+N9RkZG8Pl8uN1umpqaWF5eBmBycpJdu3bh8Xiora1lfn4+ydMQEZG1WFX4Hz58mP379wOwsLBAY2Mjfr+fYDDI8PAw/f39ANTX19PS0kJfXx+madLd3Q1AW1sb1dXVhEIhSkpK8Pv9KZqOiIisRsLw/89//kNnZyf33nsvAEeOHKGoqIjCwkLsdjter5dQKMTExAQLCwuUlpYC4PP5CIVCLC0tMTg4iNvtXtEuIiLpk3AD95aWFnbv3s3Ro0cBmJqawul0xs8bhkE4HD6j3el0Eg6HmZmZIScnB7vdvqJ9rc61F+WFzuncmu4hpI2V555sqmVyZXo9zxn++/btIz8/n/Lycl5++WUAYrEYNpst3sc0TWw221nbTz2e7uPHq5GKDdwzxUbfKDpVMmGT7EyhWiZXJtQz0Qbu5wz/YDBIJBJh586d/Pe//+Wjjz5iYmKCrKyseJ9IJIJhGOTl5RGJROLt09PTGIZBbm4us7OzRKNRsrKy4v1FRCR9zrnm/8wzz9Db20sgEOD+++/nhhtu4A9/+AOjo6OMj48TjUbp7e3F5XJRUFBAdnY2Q0NDAAQCAVwuFw6Hg7KyMoLBIAA9PT24XK7Uz0xERM4q4Zr/x2VnZ7Nnzx7q6upYXFykoqICj8cDQHt7O83NzczNzVFcXExNTQ0Ara2tNDQ00NXVRX5+Ph0dHcmdhYiIrInNNM3kLaSnUCrW/L0PBpL2fKly8LGdG35tMVUyYV01U6iWyZUJ9Uy05q9v+IqIWJDCX0TEghT+IiIWtOY3fEU+ydbPbGFzdvL/OCX7+xgLi8vMHj+R1OcUyUQKf0mKzdn2jHkDfWO/TSeyPrTsIyJiQQp/ERELUviLiFiQwl9ExIIU/iIiFqTwFxGxIIW/iIgFKfxFRCxI4S8iYkEKfxERC1pV+P/ud7/jpptuoqqqimeeeQaAgYEBvF4vlZWVdHZ2xvuOjIzg8/lwu900NTWxvLwMwOTkJLt27cLj8VBbW8v8/HwKpiMiIquRMPzfeOMNXn/9dQ4cOMBLL73E3r17eeedd2hsbMTv9xMMBhkeHqa/vx+A+vp6Wlpa6OvrwzRNuru7AWhra6O6uppQKERJSQl+vz+1MxMRkbNKGP7f+MY3ePbZZ7Hb7Rw7doxoNMrx48cpKiqisLAQu92O1+slFAoxMTHBwsICpaWlAPh8PkKhEEtLSwwODuJ2u1e0i4hIeqxq2cfhcPD4449TVVVFeXk5U1NTOJ3O+HnDMAiHw2e0O51OwuEwMzMz5OTkYLfbV7SLiEh6rPqWzvfffz9333039957L2NjY9hstvg50zSx2WzEYrFPbD/1eLqPHydyrr0oL3TJvqe91Vm1nladd6pkej0Thv97773HyZMn+epXv8qWLVuorKwkFAqRlZUV7xOJRDAMg7y8PCKRSLx9enoawzDIzc1ldnaWaDRKVlZWvP9apGID90yx0TeKBtVzo8uEDcczSSbU87w3cP/ggw9obm7m5MmTnDx5kldeeYXbbruN0dFRxsfHiUaj9Pb24nK5KCgoIDs7m6GhIQACgQAulwuHw0FZWRnBYBCAnp4eXC5XkqYoIiJrlfDKv6KigiNHjnDLLbeQlZVFZWUlVVVV5ObmUldXx+LiIhUVFXg8HgDa29tpbm5mbm6O4uJiampqAGhtbaWhoYGuri7y8/Pp6OhI7cxEROSsbKZpJm8tJYVSseyTKdsObvSXl6B6bnSZsEyRSTKhnue97CMiIhcehb+IiAUp/EVELEjhLyJiQQp/ERELUviLiFiQwl9ExIIU/iIiFqTwFxGxIIW/iIgFrfqWziKyfrZ+Zgubs5P71zPZd15dWFxm9viJpD6nrB+Fv8gGtDnbvuHvlXTwsZ1s7LvbyLlo2UdExIIU/iIiFqTwFxGxoFWF/xNPPEFVVRVVVVU8+uijAAwMDOD1eqmsrKSzszPed2RkBJ/Ph9vtpqmpieXlZQAmJyfZtWsXHo+H2tpa5ufnUzAdERFZjYThPzAwwGuvvcb+/fvp6enhH//4B729vTQ2NuL3+wkGgwwPD9Pf3w9AfX09LS0t9PX1YZom3d3dALS1tVFdXU0oFKKkpAS/35/amYmIyFklDH+n00lDQwMXXXQRDoeDK664grGxMYqKiigsLMRut+P1egmFQkxMTLCwsEBpaSkAPp+PUCjE0tISg4ODuN3uFe0iIpIeCcP/yiuvjIf52NgYf/7zn7HZbDidzngfwzAIh8NMTU2taHc6nYTDYWZmZsjJycFut69oFxGR9Fj15/z/+c9/cs899/Dzn/+crKwsxsbG4udM08RmsxGLxbDZbGe0n3o83cePEznXXpQXumR/OcfqVM/ksXItM33uqwr/oaEh7r//fhobG6mqquKNN94gEonEz0ciEQzDIC8vb0X79PQ0hmGQm5vL7Ows0WiUrKyseP+1SMUG7plio28UDapnsmVKPTOhlqlgiQ3cjx49yk9+8hPa29upqqoCYPv27YyOjjI+Pk40GqW3txeXy0VBQQHZ2dkMDQ0BEAgEcLlcOBwOysrKCAaDAPT09OByuZIxPxER+RQSXvk/9dRTLC4usmfPnnjbbbfdxp49e6irq2NxcZGKigo8Hg8A7e3tNDc3Mzc3R3FxMTU1NQC0trbS0NBAV1cX+fn5dHR0pGhKIiKSSMLwb25uprm5+RPPHThw4Iy2q666ihdffPGM9oKCAvbu3fsphigiIsmmb/iKiFiQwl9ExIIU/iIiFqTwFxGxIIW/iIgFKfxFRCxI4S8iYkEKfxERC1L4i4hYkMJfRMSCFP4iIhak8BcRsaBVb+YiIpKJtn5mC5uzkx91yd5zYWFxmdnjJ5L6nOei8BeRC9rmbDveBwPpHkZCBx/byXpuD6NlHxERC1pV+M/NzXHzzTfzwQcfADAwMIDX66WyspLOzs54v5GREXw+H263m6amJpaXlwGYnJxk165deDweamtrmZ+fT8FURERktRKG/1tvvcXtt98e37B9YWGBxsZG/H4/wWCQ4eFh+vv7Aaivr6elpYW+vj5M06S7uxuAtrY2qqurCYVClJSU4Pf7UzcjERFJKGH4d3d309raGt9w/ciRIxQVFVFYWIjdbsfr9RIKhZiYmGBhYYHS0lIAfD4foVCIpaUlBgcHcbvdK9pFRCR9Er7h+9BDD604npqawul0xo8NwyAcDp/R7nQ6CYfDzMzMkJOTg91uX9EuIiLps+ZP+8RiMWw2W/zYNE1sNttZ2089nu7jx6uxbVvOmn/mQpHsj5RZneqZPKplcq1nPdcc/nl5eUQikfhxJBLBMIwz2qenpzEMg9zcXGZnZ4lGo2RlZcX7r9WxY3PEYuaaf+5sMukPbSSynh8A+3RUz+TKlHqqlsmVzHpu2mQ750Xzmj/quX37dkZHRxkfHycajdLb24vL5aKgoIDs7GyGhoYACAQCuFwuHA4HZWVlBINBAHp6enC5XJ9yOiIikgxrvvLPzs5mz5491NXVsbi4SEVFBR6PB4D29naam5uZm5ujuLiYmpoaAFpbW2loaKCrq4v8/Hw6OjqSOwsREVmTVYf/oUOH4v9eXl7OgQMHzuhz1VVX8eKLL57RXlBQwN69ez/lEEVEJNn0DV8REQtS+IuIWJDCX0TEghT+IiIWpPAXEbEghb+IiAUp/EVELEjhLyJiQQp/ERELUviLiFiQwl9ExIIU/iIiFqTwFxGxIIW/iIgFKfxFRCxI4S8iYkHrGv4HDx7kpptuorKykueff349f7WIiJxmzds4flrhcJjOzk5efvllLrroIm677Ta++c1v8uUvf3m9hiAiIv/PuoX/wMAA3/rWt/jsZz8LgNvtJhQKcd99963q5zdtsiV9TMbntiT9OVMhFXNPBdUzuTKhnqplciWznomey2aappm033YOTz75JB999BG7d+8GYN++fRw5coRf//rX6/HrRUTkNOu25h+LxbDZ/v//iUzTXHEsIiLrZ93CPy8vj0gkEj+ORCIYhrFev15ERE6zbuH/7W9/m8OHD/Phhx9y4sQJ/vKXv+Byudbr14uIyGnW7Q3fSy+9lN27d1NTU8PS0hK33norX/va19br14uIyGnW7Q1fERHZOPQNXxERC1L4i4hYkMJfRMSCFP4iIhak8BcRsaB1+6inyLn87W9/4+jRo1RUVPDFL34x3v6nP/2JH/3oR2kcWWZ677336Ovr49///jebNm3CMAy++93vcvXVV6d7aLJB6Mpf0q69vZ3nnnuOsbExbr/9dgKBQPzcCy+8kMaRZabnn3+eBx54AICrr76a4uJiAH7xi1/w9NNPp3NosoHoyv9TmpycPOf5L3zhC+s0kszX39/P/v37sdvt3HHHHdx1111cdNFF7NixA30NZe2effZZenp62LJl5Z0s77zzTn7wgx9w1113pWlkspEo/D+le+65h7GxMQzDOCOgbDYbr7zySppGlnlOv8nfZZddxpNPPsmdd95Jbm6ubv73KdjtdpaXl89oX1hYwOFwpGFEme3mm2/mxIkTZ7Sf+nObqX/X9Q3fT2lubo7q6mpaW1u59tpr0z2cjPbEE08wMDBAQ0ND/JYfQ0ND3HfffZw8eZKhoaE0jzCzHDx4kN/+9reUl5fjdDqx2WxMTU3x+uuvs3v3bqqqqtI9xIzy7rvvcvfdd9PR0UF+fv4Z5wsKCtIwqvOn8D8PR44cYd++fdqTIAkOHz6MYRhcccUV8bajR4/y9NNP09TUlMaRZaZwOMzhw4eZmpoiFouRl5dHeXk5l156abqHlpF6eno4dOgQjz/+eLqHkjQKfxGRVZibmyMnJweAV199leuvvz7NIzo/+rSPiMgqnAp+4IJ4BaDwFxFZowthwUThLyKyRjfccEO6h3DetOYvImJBuvIXEbEghb+IiAUp/EVELEjhLyJiQQp/EREL+j9rvphMY78ooAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['sentiment'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44845551",
   "metadata": {},
   "source": [
    "From the bar graph above we discover that the majority of tweets are classified as pro the belief that climate change is man-made, followed by factual news, then neutral content and lastly tweets that show the believe that climate change is not man-made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11317d5d",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "Next we clean the message text to ensure that the data is in a usable format. First let's take a look at a message to see what kind of content we can expect to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e3492fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PolySciMajor EPA chief doesn't think carbon dioxide is main cause of global warming and.. wait, what!? https://t.co/yeLvcEFXkC via @mashable\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the first tweet\n",
    "train['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba604a",
   "metadata": {},
   "source": [
    "From the message above we observe that messages may contain web-urls, punctuation, upper and lower case characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8804da1d",
   "metadata": {},
   "source": [
    "### Removing Noise\n",
    "Next we will remove unneccesary information, as seen above. These include web-urls, punctuation and stop-words. We will also convert the text to lower-case.\n",
    "\n",
    "#### URL conversion\n",
    "\n",
    "First we will replace website URLs with the text `url-web`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a31b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern to identify URLs\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "# Replacement text for URLs\n",
    "subs_url = r'url-web'\n",
    "# Replacement\n",
    "train['message'] = train['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795aabd",
   "metadata": {},
   "source": [
    "#### Punctuation removal\n",
    "Next we will ensure text uniformity by converting all text to lower case and then removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44afc495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation to be removed: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Message text to lower case\n",
    "train['message'] = train['message'].str.lower()\n",
    "\n",
    "# Removing punctuation\n",
    "print('Punctuation to be removed:', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4826f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation\n",
    "def remove_punctuation(message):\n",
    "    \"\"\"this function removes punctuation from tweets\"\"\"\n",
    "    return ''.join([l for l in message if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1db1ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the remove_punctuation function\n",
    "train['message'] = train['message'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a36f46",
   "metadata": {},
   "source": [
    "Let's take another look at the first message to see the effect of the text clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0155eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'polyscimajor epa chief doesnt think carbon dioxide is main cause of global warming and wait what urlweb via mashable'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the first tweet\n",
    "train['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b9aa30",
   "metadata": {},
   "source": [
    "From the message above it's evident that all punctuation and unneccesary text has been removed. Next we will proceed with Feature Engineering.\n",
    "\n",
    "**REMINDER** Check if any punctuation snuck through - standard encoding on text files in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec56b444",
   "metadata": {},
   "source": [
    "Next we will: \n",
    "* split message text into single words, known as tokens.\n",
    "* apply stemming to the tokens to get to the root form of the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be47a8c",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0519c7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer object\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "# Tokenize words\n",
    "train['tokens'] = train['message'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83413827",
   "metadata": {},
   "source": [
    "Let's take another look at the first message to see the effect of tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c07ad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['polyscimajor', 'epa', 'chief', 'doesnt', 'think', 'carbon', 'dioxide', 'is', 'main', 'cause', 'of', 'global', 'warming', 'and', 'wait', 'what', 'urlweb', 'via', 'mashable']\n"
     ]
    }
   ],
   "source": [
    "print(train['tokens'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981a366",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Next we will identify the root of each token created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d73176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Stemming object\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a7c0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply the stemmer to each word\n",
    "def word_stemmer(message, stemmer):\n",
    "    \"\"\"the function applies the stemmer object to each\n",
    "        word to return the stem of the word\"\"\"\n",
    "    return [stemmer.stem(word) for word in message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1656d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['stem'] = train['tokens'].apply(word_stemmer, args=(stemmer, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a2876b",
   "metadata": {},
   "source": [
    "Let's take another look at the first message to see the effect of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bfca40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polyscimajor         --> polyscimajor\n",
      "epa                  --> epa       \n",
      "chief                --> chief     \n",
      "doesnt               --> doesnt    \n",
      "think                --> think     \n",
      "carbon               --> carbon    \n",
      "dioxide              --> dioxid    \n",
      "is                   --> is        \n",
      "main                 --> main      \n",
      "cause                --> caus      \n",
      "of                   --> of        \n",
      "global               --> global    \n",
      "warming              --> warm      \n",
      "and                  --> and       \n",
      "wait                 --> wait      \n",
      "what                 --> what      \n",
      "urlweb               --> urlweb    \n",
      "via                  --> via       \n",
      "mashable             --> mashabl   \n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(train.iloc[0]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, train.iloc[0]['stem'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa3fddf",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Next we will group words with similar meaning together using a process called lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0def7598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lemmatization object\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "097eb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply the lemmatization object to words\n",
    "def message_lemma(message, lemmatizer):\n",
    "    \"\"\" Lemmatize the tokens into words with similar meaning\"\"\"\n",
    "    return [lemmatizer.lemmatize(word) for word in message]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f1fe37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the tokens\n",
    "train['lemma'] = train['tokens'].apply(message_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e123364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polyscimajor         --> polyscimajor\n",
      "epa                  --> epa       \n",
      "chief                --> chief     \n",
      "doesnt               --> doesnt    \n",
      "think                --> think     \n",
      "carbon               --> carbon    \n",
      "dioxide              --> dioxide   \n",
      "is                   --> is        \n",
      "main                 --> main      \n",
      "cause                --> cause     \n",
      "of                   --> of        \n",
      "global               --> global    \n",
      "warming              --> warming   \n",
      "and                  --> and       \n",
      "wait                 --> wait      \n",
      "what                 --> what      \n",
      "urlweb               --> urlweb    \n",
      "via                  --> via       \n",
      "mashable             --> mashable  \n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(train.iloc[0]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, train.iloc[0]['lemma'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8b67c",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "Next well remove all of the English stopwords from the message by first creating a removal function and then applying the function to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18a1c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "547427b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
       "      <td>[it, not, like, we, lack, evid, of, anthropoge...</td>\n",
       "      <td>[it, not, like, we, lack, evidence, of, anthro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt rawstory researchers say we have three year...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[rt, rawstory, researchers, say, we, have, thr...</td>\n",
       "      <td>[rt, rawstori, research, say, we, have, three,...</td>\n",
       "      <td>[rt, rawstory, researcher, say, we, have, thre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker wired  2016 was a pivotal year in...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[todayinmaker, wired, 2016, was, a, pivotal, y...</td>\n",
       "      <td>[todayinmak, wire, 2016, was, a, pivot, year, ...</td>\n",
       "      <td>[todayinmaker, wired, 2016, wa, a, pivotal, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt soynoviodetodas its 2016 and a racist sexis...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[rt, soynoviodetodas, its, 2016, and, a, racis...</td>\n",
       "      <td>[rt, soynoviodetoda, it, 2016, and, a, racist,...</td>\n",
       "      <td>[rt, soynoviodetodas, it, 2016, and, a, racist...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221   \n",
       "1          1  its not like we lack evidence of anthropogenic...   126103   \n",
       "2          2  rt rawstory researchers say we have three year...   698562   \n",
       "3          1  todayinmaker wired  2016 was a pivotal year in...   573736   \n",
       "4          1  rt soynoviodetodas its 2016 and a racist sexis...   466954   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [its, not, like, we, lack, evidence, of, anthr...   \n",
       "2  [rt, rawstory, researchers, say, we, have, thr...   \n",
       "3  [todayinmaker, wired, 2016, was, a, pivotal, y...   \n",
       "4  [rt, soynoviodetodas, its, 2016, and, a, racis...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [it, not, like, we, lack, evid, of, anthropoge...   \n",
       "2  [rt, rawstori, research, say, we, have, three,...   \n",
       "3  [todayinmak, wire, 2016, was, a, pivot, year, ...   \n",
       "4  [rt, soynoviodetoda, it, 2016, and, a, racist,...   \n",
       "\n",
       "                                               lemma  \n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...  \n",
       "1  [it, not, like, we, lack, evidence, of, anthro...  \n",
       "2  [rt, rawstory, researcher, say, we, have, thre...  \n",
       "3  [todayinmaker, wired, 2016, wa, a, pivotal, ye...  \n",
       "4  [rt, soynoviodetodas, it, 2016, and, a, racist...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62394652",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "Next we will drop the stop words from the messages and generate a few features, including:\n",
    "* word frequecy (bag of words) - counts on word usage\n",
    "* negative words - idetifying negation words \n",
    "* rare words - looking for rarest words used\n",
    "* question - identify questions\n",
    "\n",
    "Next we'll convert the message text to a dictionary, with the word as the key and its count as the value. \n",
    "\n",
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f12038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a bag of words\n",
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\"this function takes a list of words and returns a dictionary\n",
    "        with each word as a key, and the value represents the number\n",
    "        of times that word appeared.\"\"\"\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baceb100",
   "metadata": {},
   "source": [
    "Next we create a set of dictionaries, one for each of the sentiment types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e34b984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment dictionary creation\n",
    "sentiment = {}\n",
    "for s in sentiment_labels:\n",
    "    df = train.groupby('sentiment')\n",
    "    sentiment[s] = {}\n",
    "    for row in df.get_group(s)['tokens']:\n",
    "        sentiment[s] = bag_of_words_count(row, sentiment[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf4c24",
   "metadata": {},
   "source": [
    "Next, we create a list of all the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bb5eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for s in sentiment_labels:\n",
    "    for word in sentiment[s]:\n",
    "        all_words.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fad18a",
   "metadata": {},
   "source": [
    "Next, we will create a combined bag of words dictionary for all the words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86bf0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment['all'] = {}\n",
    "for s in sentiment_labels:\n",
    "    for word in all_words:\n",
    "        if word in sentiment[s].keys():\n",
    "            if word in sentiment['all']:\n",
    "                sentiment['all'][word] += sentiment[s][word]\n",
    "            else:\n",
    "                sentiment['all'][word] = sentiment[s][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f5984",
   "metadata": {},
   "source": [
    "Next we will calculate the number of words in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d3106f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 278214 words in total\n"
     ]
    }
   ],
   "source": [
    "total_words = sum([v for v in sentiment['all'].values()])\n",
    "print('\\nThere are', total_words, 'words in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe65e3",
   "metadata": {},
   "source": [
    "Next, we will look at the distribution of words which occur less than 10 times in the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef9a6ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist([v for v in sentiment['all'].values() if v < 10], bins=10)\n",
    "plt.ylabel = ('# of words')\n",
    "plt.xlabel = ('word frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70899b",
   "metadata": {},
   "source": [
    "From the histogram above it appears a number of words appear only once. Let's get an exact count of those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "128bd7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16846 words that appear only once.\n"
     ]
    }
   ],
   "source": [
    "# count of the number of words that appear once\n",
    "once_count = len([v for v in sentiment['all'].values() if v == 1])\n",
    "print('There are', once_count, 'words that appear only once.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e759c683",
   "metadata": {},
   "source": [
    "Next, we further investigate these words by printing them out to get a sense of the kind of words they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c5ed3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are the first 50 rare words:\n",
      "\n",
      "['paymenã¢â‚¬â¦', 'milford', 'prospects', '👍🏼', 'j3ff800', 'mdrache', 'waight4noone', 'distance', 'hvsthed', 'freespeech', 'ã°å¸â€œâ·exhibit', 'achalac', 'baptcarecommunity', 'climatology', 'heterosexual', 'postcardjohn', 'tabbyterry', 'pretence', 'andylassner', 'ditacannon', 'dharmadude', 'mrgates', 'facesofchange', 'residential', 'charlesornstein', 'channelsfeeddigestcomnewsidã¢â‚¬â¦', 'incredulous', 'someday', 'bikini', 'ostriches', 'jakevig', '£8', 'magsmcm', 'slapped', 'arc…', 'dairyisscary', 'woã¢â‚¬â¦', 'wildfires…', 'crocodile', 'multiscale', 'barbarism', 'urbancommunities', 'agchatoz', 'sarahwpoljanski', 'jszilagyi97', 'cathdweeb', 'ariberman', 'ssa', 'wont…', 'myvcrew']\n"
     ]
    }
   ],
   "source": [
    "# list the words that appear once\n",
    "rare_words = [k for k, v in sentiment['all'].items() if v == 1]\n",
    "print('The following are the first 50 rare words:', end='\\n\\n')\n",
    "print(rare_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7294a94",
   "metadata": {},
   "source": [
    "Some of these words do not make sense. However before we remove them, let's establish how much these contribute to the total word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3a7f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2494 words that appear over 10 times.\n",
      "\n",
      "These account for 235570 of the total words. This constitutes approximately 85% of the total word count.\n"
     ]
    }
   ],
   "source": [
    "# count of the words appearing over 10 times\n",
    "over_ten_count = len([v for v in sentiment['all'].values() if v >= 10])\n",
    "print('There are', over_ten_count, 'words that appear over 10 times.', end='\\n\\n')\n",
    "\n",
    "# total contribution count & percentage\n",
    "over_ten_contribution = sum([v for v in sentiment['all'].values() if v >= 10])\n",
    "print('These account for', over_ten_contribution, 'of the total words', end='' )\n",
    "perc = over_ten_contribution / total_words\n",
    "print('. This constitutes approximately', str(round(perc*100,)) + '%', 'of the total word count.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdc11d",
   "metadata": {},
   "source": [
    "Therefor, we will remove roughly 15% of the words, as these are only used once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "469e72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that don't occur at least 10 times\n",
    "max_count = 10\n",
    "remaining_word_index = [k for k, v in sentiment['all'].items() if v > max_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a651123",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "Earlier we hypothesised that:\n",
    "1. Denialists may use negative terms such `lies`, `exposed`, `conspiracy`, `fake` more frequently\n",
    "2. Pro-climate change individuals may use words such as `climate`, `global warming`, `earth`, `biosphere`, `global`, `fight`, `crisis`, `threat`, `denial` more often.\n",
    "3. News media will mostly share URL links to full articles.\n",
    "\n",
    "Next we will validate the hypothesis by checking the most used terms per sentiment class. First we'll create one big dataframe with the word counts by sentiment class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a3229677",
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = []\n",
    "for s, s_bow in sentiment.items():\n",
    "    df_bow = pd.DataFrame([(k, v) for k, v in s_bow.items() if k in remaining_word_index], columns=['Word', s])\n",
    "    df_bow.set_index('Word', inplace=True)\n",
    "    hm.append(df_bow)\n",
    "    \n",
    "# create one big dataframe\n",
    "df_bow = pd.concat(hm, axis=1)\n",
    "df_bow.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce491e5",
   "metadata": {},
   "source": [
    "Next we'll highlight the top 10 words used by pro-climate change tweet authors `sentiment class: 1`. FIrst we'll extract the `sentiment class: 1` from all the sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5dd50707",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_types = [s for s in sentiment_labels if s == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "050567c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>-1</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>epa</th>\n",
       "      <td>268.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chief</th>\n",
       "      <td>66.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doesnt</th>\n",
       "      <td>718.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>234.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>carbon</th>\n",
       "      <td>96.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1      2     0    -1  all\n",
       "Word                                 \n",
       "epa     268.0  240.0  29.0  16.0  553\n",
       "chief    66.0  100.0   5.0   5.0  176\n",
       "doesnt  718.0   21.0  53.0  20.0  812\n",
       "think   234.0   16.0  81.0  29.0  360\n",
       "carbon   96.0   78.0  12.0  15.0  201"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6027c61",
   "metadata": {},
   "source": [
    "Next we'll create total word count columns for each sentiment lable `(1,2,0,-1)`. \n",
    "\n",
    "Thereafter, we'll calculate the percentage contribution of each word specific to the sentiment column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fbf6776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_types = [s for s in sentiment_labels if s == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ffde8317",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow[1] = df_bow[sentiment_types].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6ddc6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [1, 'all']:\n",
    "    df_bow[str(col)+'_perc'] = df_bow[col] / df_bow[col].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "20497557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>-1</th>\n",
       "      <th>all</th>\n",
       "      <th>1_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2</th>\n",
       "      <th>-1_perc</th>\n",
       "      <th>chi2_-1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>7346.0</td>\n",
       "      <td>3327.0</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>12897</td>\n",
       "      <td>0.053632</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.003895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <td>6968.0</td>\n",
       "      <td>3139.0</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>12212</td>\n",
       "      <td>0.050872</td>\n",
       "      <td>0.052228</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.037824</td>\n",
       "      <td>0.003972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>5796.0</td>\n",
       "      <td>2151.0</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>9722</td>\n",
       "      <td>0.042316</td>\n",
       "      <td>0.041579</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.000979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urlweb</th>\n",
       "      <td>3188.0</td>\n",
       "      <td>3627.0</td>\n",
       "      <td>754.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>8028</td>\n",
       "      <td>0.023275</td>\n",
       "      <td>0.034334</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.023621</td>\n",
       "      <td>0.003343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>4769.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.034818</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.043434</td>\n",
       "      <td>0.003387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              1       2       0     -1    all    1_perc  all_perc      chi2  \\\n",
       "Word                                                                          \n",
       "climate  7346.0  3327.0  1437.0  787.0  12897  0.053632  0.055158  0.003895   \n",
       "change   6968.0  3139.0  1370.0  735.0  12212  0.050872  0.052228  0.003972   \n",
       "rt       5796.0  2151.0  1091.0  684.0   9722  0.042316  0.041579  0.000979   \n",
       "urlweb   3188.0  3627.0   754.0  459.0   8028  0.023275  0.034334  0.003343   \n",
       "the      4769.0  1129.0   946.0  844.0   7688  0.034818  0.032880  0.003387   \n",
       "\n",
       "          -1_perc   chi2_-1  \n",
       "Word                         \n",
       "climate  0.040500  0.003895  \n",
       "change   0.037824  0.003972  \n",
       "rt       0.035200  0.000979  \n",
       "urlweb   0.023621  0.003343  \n",
       "the      0.043434  0.003387  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.sort_values(by='all', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "91c4b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate chi2\n",
    "df_bow['chi2'] = np.power((df_bow['1_perc'] - df_bow['all_perc']), 2) / df_bow['all_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f218d628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shes</th>\n",
       "      <td>0.004570</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.001259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>believe</th>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.001141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doesnt</th>\n",
       "      <td>0.005242</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.008703</td>\n",
       "      <td>0.006441</td>\n",
       "      <td>0.000794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.000726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>die</th>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.000663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stephenschlegel</th>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.000656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>husband</th>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.000627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thinking</th>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.000599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensanders</th>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.000463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   1_perc  all_perc      chi2\n",
       "Word                                         \n",
       "shes             0.004570  0.002720  0.001259\n",
       "believe          0.007308  0.004935  0.001141\n",
       "doesnt           0.005242  0.003473  0.000901\n",
       "we               0.008703  0.006441  0.000794\n",
       "going            0.004307  0.002865  0.000726\n",
       "die              0.002877  0.001788  0.000663\n",
       "stephenschlegel  0.002241  0.001313  0.000656\n",
       "husband          0.002278  0.001356  0.000627\n",
       "thinking         0.002460  0.001510  0.000599\n",
       "sensanders       0.001818  0.001103  0.000463"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['1_perc', 'all_perc', 'chi2']][df_bow['1_perc'] > df_bow['all_perc']].sort_values(by='chi2', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "db21acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_types = [s for s in sentiment_labels if s == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "27e29a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow[-1] = df_bow[sentiment_types].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "dc41f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [-1, 'all']:\n",
    "    df_bow[str(col)+'_perc'] = df_bow[col] / df_bow[col].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "dbc49e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>-1</th>\n",
       "      <th>all</th>\n",
       "      <th>1_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2</th>\n",
       "      <th>-1_perc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>climate</th>\n",
       "      <td>7346.0</td>\n",
       "      <td>3327.0</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>12897</td>\n",
       "      <td>0.053632</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <td>6968.0</td>\n",
       "      <td>3139.0</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>735.0</td>\n",
       "      <td>12212</td>\n",
       "      <td>0.050872</td>\n",
       "      <td>0.052228</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.037824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rt</th>\n",
       "      <td>5796.0</td>\n",
       "      <td>2151.0</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>9722</td>\n",
       "      <td>0.042316</td>\n",
       "      <td>0.041579</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>urlweb</th>\n",
       "      <td>3188.0</td>\n",
       "      <td>3627.0</td>\n",
       "      <td>754.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>8028</td>\n",
       "      <td>0.023275</td>\n",
       "      <td>0.034334</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>0.023621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>4769.0</td>\n",
       "      <td>1129.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.034818</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.043434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              1       2       0     -1    all    1_perc  all_perc      chi2  \\\n",
       "Word                                                                          \n",
       "climate  7346.0  3327.0  1437.0  787.0  12897  0.053632  0.055158  0.000042   \n",
       "change   6968.0  3139.0  1370.0  735.0  12212  0.050872  0.052228  0.000035   \n",
       "rt       5796.0  2151.0  1091.0  684.0   9722  0.042316  0.041579  0.000013   \n",
       "urlweb   3188.0  3627.0   754.0  459.0   8028  0.023275  0.034334  0.003562   \n",
       "the      4769.0  1129.0   946.0  844.0   7688  0.034818  0.032880  0.000114   \n",
       "\n",
       "          -1_perc  \n",
       "Word               \n",
       "climate  0.040500  \n",
       "change   0.037824  \n",
       "rt       0.035200  \n",
       "urlweb   0.023621  \n",
       "the      0.043434  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow.sort_values(by='all', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a6149390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate chi2\n",
    "df_bow['chi2_-1'] = np.power((df_bow['-1_perc'] - df_bow['all_perc']), 2) / df_bow['all_perc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "08a1c828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1_perc</th>\n",
       "      <th>all_perc</th>\n",
       "      <th>chi2_-1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>realdonaldtrump</th>\n",
       "      <td>0.003602</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.004864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.027069</td>\n",
       "      <td>0.018677</td>\n",
       "      <td>0.003771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.043434</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.003387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concept</th>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.003157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturing</th>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shes</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.002618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>0.004323</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.002258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created</th>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.002075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>0.008131</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.001892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doesnt</th>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.001719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  -1_perc  all_perc   chi2_-1\n",
       "Word                                         \n",
       "realdonaldtrump  0.003602  0.001193  0.004864\n",
       "is               0.027069  0.018677  0.003771\n",
       "the              0.043434  0.032880  0.003387\n",
       "concept          0.001338  0.000325  0.003157\n",
       "manufacturing    0.001287  0.000308  0.003110\n",
       "shes             0.000051  0.002720  0.002618\n",
       "no               0.004323  0.002130  0.002258\n",
       "created          0.001441  0.000462  0.002075\n",
       "it               0.008131  0.005042  0.001892\n",
       "doesnt           0.001029  0.003473  0.001719"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bow[['-1_perc', 'all_perc', 'chi2_-1']][df_bow['1_perc'] > df_bow['all_perc']].sort_values(by='chi2_-1', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b09b9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate word frequency\n",
    "def gen_freq(text):\n",
    "    #Will store the list of words\n",
    "    word_list = []\n",
    "\n",
    "    #Loop over all the tweets and extract words into word_list\n",
    "    for tw_words in text.split():\n",
    "        word_list.extend(tw_words)\n",
    "\n",
    "    #Create word frequencies using word_list\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "#Check whether a negation term is present in the text\n",
    "def any_neg(words):\n",
    "    for word in words:\n",
    "        if word in ['n', 'no', 'non', 'not'] or re.search(r\"\\wn't\", word):\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Check whether one of the 100 rare words is present in the text\n",
    "def any_rare(words, rare_100):\n",
    "    for word in words:\n",
    "        if word in rare_100:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Check whether prompt words are present\n",
    "def is_question(words):\n",
    "    for word in words:\n",
    "        if word in ['when', 'what', 'how', 'why', 'who']:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2b2632d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = gen_freq(train.message.str)\n",
    "#100 most rare words in the dataset\n",
    "rare_100 = word_freq[-100:]\n",
    "\n",
    "#Number of words in a tweet\n",
    "train['word_count'] = train.message.str.split().apply(lambda x: len(x))\n",
    "\n",
    "#Negation present or not\n",
    "train['any_neg'] = train.message.str.split().apply(lambda x: any_neg(x))\n",
    "\n",
    "#Prompt present or not\n",
    "train['is_question'] = train.message.str.split().apply(lambda x: is_question(x))\n",
    "\n",
    "#Any of the most 100 rare words present or not\n",
    "train['any_rare'] = train.message.str.split().apply(lambda x: any_rare(x, rare_100))\n",
    "\n",
    "#Character count of the tweet\n",
    "train['char_count'] = train.message.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "bd4aed38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem</th>\n",
       "      <th>lemma</th>\n",
       "      <th>word_count</th>\n",
       "      <th>any_neg</th>\n",
       "      <th>is_question</th>\n",
       "      <th>any_rare</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>polyscimajor epa chief doesnt think carbon dio...</td>\n",
       "      <td>625221</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>[polyscimajor, epa, chief, doesnt, think, carb...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>its not like we lack evidence of anthropogenic...</td>\n",
       "      <td>126103</td>\n",
       "      <td>[its, not, like, we, lack, evidence, of, anthr...</td>\n",
       "      <td>[it, not, like, we, lack, evid, of, anthropoge...</td>\n",
       "      <td>[it, not, like, we, lack, evidence, of, anthro...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>rt rawstory researchers say we have three year...</td>\n",
       "      <td>698562</td>\n",
       "      <td>[rt, rawstory, researchers, say, we, have, thr...</td>\n",
       "      <td>[rt, rawstori, research, say, we, have, three,...</td>\n",
       "      <td>[rt, rawstory, researcher, say, we, have, thre...</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>todayinmaker wired  2016 was a pivotal year in...</td>\n",
       "      <td>573736</td>\n",
       "      <td>[todayinmaker, wired, 2016, was, a, pivotal, y...</td>\n",
       "      <td>[todayinmak, wire, 2016, was, a, pivot, year, ...</td>\n",
       "      <td>[todayinmaker, wired, 2016, wa, a, pivotal, ye...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>rt soynoviodetodas its 2016 and a racist sexis...</td>\n",
       "      <td>466954</td>\n",
       "      <td>[rt, soynoviodetodas, its, 2016, and, a, racis...</td>\n",
       "      <td>[rt, soynoviodetoda, it, 2016, and, a, racist,...</td>\n",
       "      <td>[rt, soynoviodetodas, it, 2016, and, a, racist...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid  \\\n",
       "0          1  polyscimajor epa chief doesnt think carbon dio...   625221   \n",
       "1          1  its not like we lack evidence of anthropogenic...   126103   \n",
       "2          2  rt rawstory researchers say we have three year...   698562   \n",
       "3          1  todayinmaker wired  2016 was a pivotal year in...   573736   \n",
       "4          1  rt soynoviodetodas its 2016 and a racist sexis...   466954   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [its, not, like, we, lack, evidence, of, anthr...   \n",
       "2  [rt, rawstory, researchers, say, we, have, thr...   \n",
       "3  [todayinmaker, wired, 2016, was, a, pivotal, y...   \n",
       "4  [rt, soynoviodetodas, its, 2016, and, a, racis...   \n",
       "\n",
       "                                                stem  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...   \n",
       "1  [it, not, like, we, lack, evid, of, anthropoge...   \n",
       "2  [rt, rawstori, research, say, we, have, three,...   \n",
       "3  [todayinmak, wire, 2016, was, a, pivot, year, ...   \n",
       "4  [rt, soynoviodetoda, it, 2016, and, a, racist,...   \n",
       "\n",
       "                                               lemma  word_count  any_neg  \\\n",
       "0  [polyscimajor, epa, chief, doesnt, think, carb...          19        0   \n",
       "1  [it, not, like, we, lack, evidence, of, anthro...          10        1   \n",
       "2  [rt, rawstory, researcher, say, we, have, thre...          19        0   \n",
       "3  [todayinmaker, wired, 2016, wa, a, pivotal, ye...          14        0   \n",
       "4  [rt, soynoviodetodas, it, 2016, and, a, racist...          18        0   \n",
       "\n",
       "   is_question  any_rare  char_count  \n",
       "0            1         0         116  \n",
       "1            0         0          61  \n",
       "2            0         0         108  \n",
       "3            0         0          79  \n",
       "4            0         0         114  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "db6f9475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# create a CountVectorizer object\\nvect = CountVectorizer(stop_words='english', \\n                       ngram_range=(1, 2), \\n                       max_df=0.5,\\n                       min_df=2)\\nvect.fit(train['message'])\""
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# create a CountVectorizer object\n",
    "vect = CountVectorizer(stop_words='english', \n",
    "                       ngram_range=(1, 2), \n",
    "                       max_df=0.5,\n",
    "                       min_df=2)\n",
    "vect.fit(train['message'])\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5009741",
   "metadata": {},
   "source": [
    "### Model building\n",
    "Next we will train a Machine Learning model on the same. I will be using a Naive Bayes classifier from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7221e5c",
   "metadata": {},
   "source": [
    "#### Splitting the dataset into Train-Test split\n",
    "\n",
    " - The dataset is split into train and test sets so that we can evaluate our model's performance on unseen data.\n",
    " - The model will only be trained on the `train` set and will make predictions on the `test` set whose data points the model has never seen. This will make sure that we have a proper way to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2e7e8fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into Train-Test split\n",
    "X = train[['word_count', 'any_neg', 'any_rare', 'char_count', 'is_question']]\n",
    "y = train.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "287504d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# initialise model object\n",
    "model = GaussianNB()\n",
    "\n",
    "# fit model to train dataset\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# make a prediction\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c8c8f",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "cd6fad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 56 %\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "print('Accuracy:', round(accuracy_score(y_test, pred) * 100,), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1dba0996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/stanfordgibson/climate-change-sentiment/cb820c2894314681b057d04e8dbe8713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"uSjPrOwXQdE1GLSEQuhuEJ1QQ\",\n",
    "    project_name=\"climate-change-sentiment\",\n",
    "    workspace=\"stanfordgibson\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
